## sigmoid 和 softmax
`sigmoid`和`softmax`函数都是在机器学习和深度学习中常用的激活函数，它们用于将模型输出转换为概率分布，但它们的应用场景和公式有所不同。

1. **Sigmoid函数**:
    - **公式**：$S(x)=\frac{1}{1 + e^{-x}}$​
    - **输出**：Sigmoid函数的输出范围在0到1之间，可以被解释为单个事件发生的概率。
    - **应用场景**：Sigmoid主要用于二分类问题，在输出层可以将线性输出转换为概率值。例如，在逻辑回归中，sigmoid函数用于预测一个实例属于某个类别的概率。
2. **Softmax函数**:
    - **公式**：$S(y_i)=\frac{e^{y_i}}{\sum_{j}e^{y_i}}$​​
    - **输出**：Softmax函数将一个含有n个数的向量转换成一个含有n个概率值的向量，这些概率值的总和为1。
    - **应用场景**：Softmax通常用于多分类问题的输出层，可以给出一个对象属于每个类别的概率。例如，在神经网络中，softmax函数常用于最后一层来表示多个类别的概率分布。

总的来说，当我们处理的是二分类问题时，倾向于使用sigmoid函数；而当我们处理的是多分类问题时，则倾向于使用softmax函数。